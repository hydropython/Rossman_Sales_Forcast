{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\10 ACADAMY KIFIYA\\Week_4\\Rossman_Sales_Forcast\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.abspath('../scripts'))\n",
    "\n",
    "from Classical_MLAs import SalesPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "---\n",
    "1. feature_engineering()\n",
    "2. handle_missing_values()\n",
    "3. encode_categorical()\n",
    "4. scale_numeric_features()\n",
    "5. see X_train_scaled, X_val_scaled, y_train, y_val \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the datasets\n",
    "# Load datasets\n",
    "train_file = '../Data/train.csv'  # Adjust this path as necessary\n",
    "df_test = pd.read_csv('../Data/test.csv')  # Load the test CSV\n",
    "df_sample_submission = pd.read_csv('../Data/sample_submission.csv')  # Load the sample submission file\n",
    "submission_file = '../Data/submission.csv'  # Specify your submission file path\n",
    "# Merge test dataset with sample submission to align IDs\n",
    "df_test_merge = df_test.merge(df_sample_submission[['Id', 'Sales']], on='Id', how='left')\n",
    "target_col = 'Sales'  # Target column is 'Sales'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train data shape: (1017209, 9)\n",
      "Train data types:\n",
      " Store             int64\n",
      "DayOfWeek         int64\n",
      "Date             object\n",
      "Sales             int64\n",
      "Customers         int64\n",
      "Open              int64\n",
      "Promo             int64\n",
      "StateHoliday     object\n",
      "SchoolHoliday     int64\n",
      "dtype: object\n",
      "Missing values in train data:\n",
      " Store            0\n",
      "DayOfWeek        0\n",
      "Date             0\n",
      "Sales            0\n",
      "Customers        0\n",
      "Open             0\n",
      "Promo            0\n",
      "StateHoliday     0\n",
      "SchoolHoliday    0\n",
      "dtype: int64\n",
      "Loaded test data shape: (41088, 8)\n"
     ]
    }
   ],
   "source": [
    "sales_predictor = SalesPredictor(train_file, df_test, 'Sales')\n",
    "rf_predictions, rf_model = sales_predictor .run_random_forest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_predictor.visualize_shap_feature_importance(rf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_predictor.visualize_predictions(sales_predictor.y_train, rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train data shape: (1017209, 9)\n",
      "Train data types:\n",
      " Store             int64\n",
      "DayOfWeek         int64\n",
      "Date             object\n",
      "Sales             int64\n",
      "Customers         int64\n",
      "Open              int64\n",
      "Promo             int64\n",
      "StateHoliday     object\n",
      "SchoolHoliday     int64\n",
      "dtype: object\n",
      "Missing values in train data:\n",
      " Store            0\n",
      "DayOfWeek        0\n",
      "Date             0\n",
      "Sales            0\n",
      "Customers        0\n",
      "Open             0\n",
      "Promo            0\n",
      "StateHoliday     0\n",
      "SchoolHoliday    0\n",
      "dtype: int64\n",
      "Loaded test data shape: (41088, 8)\n",
      "Running Random Forest...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Specify data types while loading the CSV\n",
    "dtype_dict = {\n",
    "    'Store': 'int64',\n",
    "    'DayOfWeek': 'int64',\n",
    "    'Date': 'object',\n",
    "    'Sales': 'int64',\n",
    "    'Customers': 'int64',\n",
    "    'Open': 'int64',\n",
    "    'Promo': 'int64',\n",
    "    'StateHoliday': 'str',  # Use str to handle mixed types\n",
    "    'SchoolHoliday': 'int64'\n",
    "}\n",
    "\n",
    "def load_and_merge_data(train_path, test_df, target_column):\n",
    "    train_df = pd.read_csv(train_path, dtype=dtype_dict, low_memory=False)\n",
    "    \n",
    "    print(\"Loaded train data shape:\", train_df.shape)\n",
    "    print(\"Train data types:\\n\", train_df.dtypes)\n",
    "\n",
    "    # Check for missing values\n",
    "    print(\"Missing values in train data:\\n\", train_df.isnull().sum())\n",
    "\n",
    "    # Define X and y\n",
    "    X = train_df.drop(columns=[target_column])\n",
    "    y = train_df[target_column]\n",
    "    \n",
    "    # Ensure test_df is already a DataFrame\n",
    "    X_test = test_df.drop(columns=['Sales'], errors='ignore')  # Drop Sales if it exists\n",
    "    print(\"Loaded test data shape:\", X_test.shape)\n",
    "\n",
    "    return X, y, X_test\n",
    "\n",
    "def preprocess_data(X_train, X_test):\n",
    "    # Ensure both datasets have the same columns\n",
    "    missing_cols = set(X_train.columns) - set(X_test.columns)\n",
    "    \n",
    "    # If any columns are missing in the test set, add them with default values\n",
    "    for col in missing_cols:\n",
    "        X_test[col] = 0  # or use an appropriate default value, such as the mean or median\n",
    "    \n",
    "    # Identify numeric and categorical columns\n",
    "    numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Preprocessing for numeric data\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())])\n",
    "    \n",
    "    # Preprocessing for categorical data\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    \n",
    "    # Bundle preprocessing for numeric and categorical data\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)])\n",
    "    \n",
    "    # Fit the preprocessor on the training data and transform both training and test data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Return processed data and feature names\n",
    "    return X_train_processed, X_test_processed, preprocessor.get_feature_names_out()\n",
    "def run_random_forest(X_train, y_train, X_test):\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_predictions = rf_model.predict(X_test)\n",
    "    return rf_predictions, rf_model\n",
    "\n",
    "def run_gradient_boosting(X_train, y_train, X_test):\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    gb_predictions = gb_model.predict(X_test)\n",
    "    return gb_predictions, gb_model\n",
    "\n",
    "def run_xgboost(X_train, y_train, X_test):\n",
    "    xg_model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "    xg_model.fit(X_train, y_train)\n",
    "    xg_predictions = xg_model.predict(X_test)\n",
    "    return xg_predictions, xg_model\n",
    "\n",
    "def visualize_feature_importance(model, feature_names):\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importances = model.feature_importances_\n",
    "    else:\n",
    "        feature_importances = model.get_score(importance_type='weight')\n",
    "        feature_importances = np.array([feature_importances.get(name, 0) for name in feature_names])\n",
    "\n",
    "    indices = np.argsort(feature_importances)[::-1]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(\"Feature Importances\")\n",
    "    plt.barh(range(len(feature_importances)), feature_importances[indices], align=\"center\")\n",
    "    plt.yticks(range(len(feature_importances)), [feature_names[i] for i in indices])\n",
    "    plt.xlabel(\"Relative Importance\")\n",
    "    plt.show()\n",
    "\n",
    "def visualize_predictions(y_true, y_pred):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=y_true, y=y_pred)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "    plt.xlabel(\"Actual Sales\")\n",
    "    plt.ylabel(\"Predicted Sales\")\n",
    "    plt.title(\"Actual vs Predicted Sales\")\n",
    "    plt.show()\n",
    "\n",
    "def run_all_models(train_path, test_df, submission_file, target_col):\n",
    "    X_train, y_train, X_test = load_and_merge_data(train_path, test_df, target_col)\n",
    "    \n",
    "    # Preprocess data\n",
    "    X_train, X_test, feature_names = preprocess_data(X_train, X_test)\n",
    "    \n",
    "    # Running Random Forest\n",
    "    print(\"Running Random Forest...\")\n",
    "    rf_predictions, rf_model = run_random_forest(X_train, y_train, X_test)\n",
    "    visualize_feature_importance(rf_model, feature_names)\n",
    "\n",
    "    # Running Gradient Boosting\n",
    "    print(\"Running Gradient Boosting...\")\n",
    "    gb_predictions, gb_model = run_gradient_boosting(X_train, y_train, X_test)\n",
    "    visualize_feature_importance(gb_model, feature_names)\n",
    "\n",
    "    # Running XGBoost\n",
    "    print(\"Running XGBoost...\")\n",
    "    xg_predictions, xg_model = run_xgboost(X_train, y_train, X_test)\n",
    "    visualize_feature_importance(xg_model, feature_names)\n",
    "\n",
    "    # Save predictions to submission file\n",
    "    submission_df = pd.DataFrame({'Id': test_df['Id'], 'Sales': rf_predictions})  # You can choose which model's predictions to save\n",
    "    submission_df.to_csv(submission_file, index=False)\n",
    "\n",
    "\n",
    "# Load the datasets\n",
    "# Load datasets\n",
    "train_file = '../Data/train.csv'  # Adjust this path as necessary\n",
    "df_test = pd.read_csv('../Data/test.csv')  # Load the test CSV\n",
    "df_sample_submission = pd.read_csv('../Data/sample_submission.csv')  # Load the sample submission file\n",
    "submission_file = '../Data/submission.csv'  # Specify your submission file path\n",
    "# Merge test dataset with sample submission to align IDs\n",
    "df_test_merge = df_test.merge(df_sample_submission[['Id', 'Sales']], on='Id', how='left')\n",
    "target_col = 'Sales'  # Target column is 'Sales'\n",
    "run_all_models(train_file, df_test_merge, submission_file, 'Sales') \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
